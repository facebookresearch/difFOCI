{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVR\n",
    "from data.data_loading import *\n",
    "from warnings import simplefilter\n",
    "\n",
    "simplefilter(action='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=12345):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "seed_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = simulated_dataset # toy_1, toy_2, toy_3, simulated_dataset\n",
    "scaler = StandardScaler()\n",
    "split_ratios = [0.75, 0.15, 0.10]\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = scaled_data(loader, scaler, split_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean pred. and full pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mean_pred = np.mean(y_train) * np.ones(y_test.shape)\n",
    "mean_loss = np.mean((y_test - y_mean_pred)**2)\n",
    "print(f'Mean Prediction Loss: {mean_loss}')\n",
    "\n",
    "reg = SVR(C=1.0, epsilon=0.2).fit(X_train, y_train)\n",
    "y_hat_full = reg.predict(X_test)\n",
    "full_loss = np.mean((y_test - y_hat_full)**2)\n",
    "print(f'Full Model Loss: {full_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "for perc in [0.25, 0.5, 0.75]:\n",
    "    print(\"-----------\", perc, \"-----------\")\n",
    "    n_components = np.min((X_train.shape[0], int(\n",
    "        X_train.shape[1]*perc), X_test.shape[0]))\n",
    "    estims = [umap.UMAP(n_components=n_components), PCA(n_components=n_components)]\n",
    "\n",
    "    for estim, est_n in zip(estims, [\"UMAP\", \"LDA\", \"PCA\"]):\n",
    "        X_transf_train = estim.fit_transform(X_train, y=y_train)\n",
    "        X_transf_test = estim.transform(X_test)\n",
    "\n",
    "        reg = SVR(C=1.0, epsilon=0.2).fit(X_transf_train, y_train)\n",
    "\n",
    "        yhat_test = reg.predict(X_transf_test)\n",
    "        test_loss = np.mean((y_test - y_mean_pred)**2)\n",
    "        print(est_n, \": \", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_selection\n",
    "\n",
    "estims_names = ['GenericUnivariateSelect-f', 'SelectPercentile-f', 'SelectFpr-f', 'SelectFdr-f', 'SelectFwe-f', 'SelectKBest-f25', 'SelectKBest-f50', 'SelectKBest-f75']\n",
    "score_func = feature_selection.f_classif\n",
    "\n",
    "f1 = feature_selection.GenericUnivariateSelect(score_func=score_func, param=1.) # Univariate feature selector with configurable strategy.\n",
    "f2 = feature_selection.SelectPercentile(score_func=score_func) # Select features according to a percentile of the highest scores.\n",
    "f3 = feature_selection.SelectFpr(score_func=score_func, alpha=5e-2) # Filter: Select the pvalues below alpha based on a FPR test.\n",
    "f4 = feature_selection.SelectFdr(score_func=score_func, alpha=.975) # Filter: Select the p-values for an estimated false discovery rate.\n",
    "f5 = feature_selection.SelectFwe(score_func=score_func, alpha=.975) # Filter: Select the p-values corresponding to Family-wise error rate.\n",
    "f6 = feature_selection.SelectKBest(score_func=score_func, k=int(X_train.shape[-1]*0.25)) # Select features according to the k highest scores.\n",
    "f7 = feature_selection.SelectKBest(score_func=score_func, k=int(X_train.shape[-1]*0.5)) # Select features according to the k highest scores.\n",
    "f8 = feature_selection.SelectKBest(score_func=score_func, k=int(X_train.shape[-1]*0.75)) # Select features according to the k highest scores.\n",
    "\n",
    "estims = [f1, f2, f3, f4, f5, f6, f7, f8]\n",
    "\n",
    "X_cut = []\n",
    "a = []\n",
    "for estim in estims:\n",
    "    X_train_transf = estim.fit_transform(X_train, y_train)\n",
    "    X_val_transf = estim.transform(X_val)\n",
    "    X_test_transf = estim.transform(X_test)        \n",
    "    \n",
    "    try:\n",
    "        reg = SVR(C=1.0, epsilon=0.2).fit(X_train_transf, y_train)\n",
    "        yhat = reg.predict(X_test_transf).squeeze()\n",
    "        print(np.mean((y_test - yhat)**2),str(estim)[:10])\n",
    "\n",
    "    except:\n",
    "        print(\"no feature selected\", str(estim)[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xicorpy import select_features_using_foci\n",
    "selected = select_features_using_foci(y_train, X_train)\n",
    "\n",
    "X_train_transf_foci = X_train[:,selected]\n",
    "X_test_transf_foci = X_test[:,selected]\n",
    "# SVR\n",
    "model = SVR(C=1.0, epsilon=0.2)\n",
    "model.fit(X_train_transf_foci, y_train)\n",
    "yhat = model.predict(X_test_transf_foci).squeeze()\n",
    "print(np.mean((y_test - yhat)**2), \"FOCI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# difFOCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from difFOCI._utils import one_layer_net, two_layer_net\n",
    "from difFOCI.conditional_dependence import conditional_dependence\n",
    "\n",
    "p = X_train.shape[1]\n",
    "lr = 1e-1\n",
    "wd = 1e-1\n",
    "num_epochs = 1000\n",
    "\n",
    "version = \"vec\" # vec, nn\n",
    "\n",
    "if version == \"vec\":\n",
    "    param = torch.tensor(torch.normal(1, .1, (1, p)), requires_grad=True)\n",
    "    optimizer = optim.Adam([param], lr=lr, weight_decay=wd)\n",
    "else:\n",
    "    param = one_layer_net(p, 10, 1) # one_layer_net(p, d, 1), two_layer_net(p,d1,d2,1)\n",
    "    optimizer = optim.Adam(param.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "X_train, y_train = torch.Tensor(X_train), torch.Tensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(param, version):\n",
    "    if version == \"vec\":\n",
    "        x_ = param * X_train\n",
    "    else:\n",
    "        x_ = param(X_train)\n",
    "    return -conditional_dependence(x=x_, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "        loss = criterion(param, version)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if version == \"vec\":\n",
    "    np_param = param.detach().numpy().squeeze()\n",
    "    X_train = X_train.numpy()\n",
    "    X_train_transf = (np_param * X_train)[:, np.argwhere(np.abs(np_param) > 0.1).squeeze()]\n",
    "    X_val_transf = (np_param * X_val)[:, np.argwhere(np.abs(np_param) > 0.1).squeeze()]\n",
    "    X_test_transf = (np_param * X_test)[:, np.argwhere(np.abs(np_param) > 0.1).squeeze()]\n",
    "else:\n",
    "    X_val, X_test = torch.Tensor(X_val), torch.Tensor(X_test)\n",
    "    X_train_transf = (param(X_train)).detach().numpy()\n",
    "    X_val_transf = (param(X_val)).detach().numpy()\n",
    "    X_test_transf = (param(X_test)).detach().numpy()\n",
    " \n",
    "\n",
    "if len(X_train_transf.shape) == 1:\n",
    "    X_train_transf = X_train_transf.reshape(-1, 1)\n",
    "    X_val_transf = X_train_transf.reshape(-1, 1)\n",
    "    X_test_transf = X_test_transf.reshape(-1, 1)\n",
    "\n",
    "y_train = y_train.numpy()\n",
    "reg = SVR(C=1.0, epsilon=0.2).fit(X_train_transf, y_train)\n",
    "\n",
    "train_loss = np.mean((reg.predict(X_train_transf)-y_train)**2)\n",
    "val_loss = np.mean((reg.predict(X_val_transf)-y_val)**2)\n",
    "test_loss = np.mean((reg.predict(X_test_transf)-y_test)**2)\n",
    "\n",
    "print(\"Train Loss: \", train_loss)\n",
    "print(\"Val Loss: \", val_loss)\n",
    "print(\"Test Loss: \", test_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
