{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fairness tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_loading import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(action='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=12345):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "seed_torch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = load_income # load_bank_marketing, load_student_data, load_employment, load_income\n",
    "scaler = StandardScaler()\n",
    "split_ratios = [0.75, 0.15, 0.10]\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, group_train, group_val, group_test = scaled_data_fairness(loader, scaler, split_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, group_train, group_val, group_test = map(lambda x: x[:500], [X_train, X_val, X_test, y_train, y_val, y_test, group_train, group_val, group_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "for perc in [0.25, 0.5, 0.75]:\n",
    "    print(\"-----------\", perc, \"--------\")\n",
    "    n_components = np.min((X_train.shape[0], int(\n",
    "        X_train.shape[1]*perc), X_test.shape[0]))\n",
    "    estims = [umap.UMAP(n_components=n_components), LDA(),\n",
    "              PCA(n_components=n_components)]\n",
    "\n",
    "    for estim, est_n in zip(estims, [\"UMAP\", \"LDA\", \"PCA\"]):\n",
    "        X_transf_train = estim.fit_transform(X_train, y=y_train)\n",
    "        X_transf_test = estim.transform(X_test)\n",
    "\n",
    "        reg = LogisticRegression().fit(X_transf_train, y_train)\n",
    "\n",
    "        yhat_test = reg.predict(X_transf_test)\n",
    "        log_test_loss = log_loss(y_test, yhat_test)\n",
    "        print(est_n, \": \", log_test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_selection\n",
    "\n",
    "estims_names = ['GenericUnivariateSelect-f', 'SelectPercentile-f', 'SelectFpr-f', 'SelectFdr-f', 'SelectFwe-f', 'SelectKBest-f25', 'SelectKBest-f50', 'SelectKBest-f75']\n",
    "score_func = feature_selection.f_classif\n",
    "\n",
    "f1 = feature_selection.GenericUnivariateSelect(score_func=score_func, param=1.) # Univariate feature selector with configurable strategy.\n",
    "f2 = feature_selection.SelectPercentile(score_func=score_func) # Select features according to a percentile of the highest scores.\n",
    "f3 = feature_selection.SelectFpr(score_func=score_func, alpha=5e-2) # Filter: Select the pvalues below alpha based on a FPR test.\n",
    "f4 = feature_selection.SelectFdr(score_func=score_func, alpha=.975) # Filter: Select the p-values for an estimated false discovery rate.\n",
    "f5 = feature_selection.SelectFwe(score_func=score_func, alpha=.975) # Filter: Select the p-values corresponding to Family-wise error rate.\n",
    "f6 = feature_selection.SelectKBest(score_func=score_func, k=int(X_train.shape[-1]*0.25)) # Select features according to the k highest scores.\n",
    "f7 = feature_selection.SelectKBest(score_func=score_func, k=int(X_train.shape[-1]*0.5)) # Select features according to the k highest scores.\n",
    "f8 = feature_selection.SelectKBest(score_func=score_func, k=int(X_train.shape[-1]*0.75)) # Select features according to the k highest scores.\n",
    "\n",
    "estims = [f1, f2, f3, f4, f5, f6, f7, f8]\n",
    "\n",
    "X_cut = []\n",
    "a = []\n",
    "for estim in estims:\n",
    "    X_train_transf = estim.fit_transform(X_train, y_train)\n",
    "    X_val_transf = estim.transform(X_val)\n",
    "    X_test_transf = estim.transform(X_test)        \n",
    "    model = LogisticRegression()\n",
    "    \n",
    "    try:\n",
    "        model.fit(X_train_transf, y_train)\n",
    "        yhat = model.predict(X_test_transf).squeeze()\n",
    "        print(log_loss(y_test, yhat),str(estim)[:10])\n",
    "        \n",
    "        # For Religious data\n",
    "        # yhat = model.predict_proba(X_test_transf).squeeze()\n",
    "        # print(log_loss(y_test, yhat,labels=[0, 1, 2, 3, 4,5,6,7]),str(estim)[:10])\n",
    "\n",
    "\n",
    "    except:\n",
    "        print(\"no feature selected\", str(estim)[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xicorpy import select_features_using_foci\n",
    "selected = select_features_using_foci(y_train.squeeze(), X_train)\n",
    "\n",
    "X_train_transf_foci = X_train[:,selected]\n",
    "X_test_transf_foci = X_test[:,selected]\n",
    "# SVR\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_transf_foci, y_train)\n",
    "yhat = model.predict(X_test_transf_foci).squeeze()\n",
    "print(log_loss(y_test, yhat), \"FOCI\")\n",
    "# For Religious data\n",
    "# yhat = model.predict_proba(X_test_transf).squeeze()\n",
    "# print(log_loss(y_test, yhat,labels=[0, 1, 2, 3, 4, 5, 6, 7]), \"FOCI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# difFOCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from difFOCI._utils import one_layer_net, two_layer_net\n",
    "from difFOCI.conditional_dependence import conditional_dependence, conditional_dependence_with_x\n",
    "\n",
    "p = X_train.shape[1]\n",
    "lr = 1e-1\n",
    "wd = 1e-1\n",
    "num_epochs = 1\n",
    "\n",
    "version = \"dF1\" # dF1, dF3\n",
    "\n",
    "param = torch.tensor(torch.normal(1, .1, (1, p)), requires_grad=True)\n",
    "optimizer = optim.Adam([param], lr=lr, weight_decay=wd)\n",
    "\n",
    "X_train, y_train, group_train = torch.Tensor(X_train), torch.Tensor(y_train).squeeze(), torch.Tensor(group_train)\n",
    "if group_train.ndim == 1:\n",
    "    group_train = group_train.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(param, version):\n",
    "    x_ = param * X_train\n",
    "    if version == \"dF1\":\n",
    "        return -conditional_dependence(x=x_, y=y_train)\n",
    "    else:\n",
    "\n",
    "        return -conditional_dependence_with_x(z=x_, y=y_train, x=group_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "        loss = criterion(param, version)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_param = param.detach().numpy().squeeze()\n",
    "X_train = X_train.numpy()\n",
    "X_train_transf = (np_param * X_train)[:, np.argwhere(np.abs(np_param) > 0.1).squeeze()]\n",
    "X_val_transf = (np_param * X_val)[:, np.argwhere(np.abs(np_param) > 0.1).squeeze()]\n",
    "X_test_transf = (np_param * X_test)[:, np.argwhere(np.abs(np_param) > 0.1).squeeze()]\n",
    "\n",
    " \n",
    "if len(X_train_transf.shape) == 1:\n",
    "    X_train_transf = X_train_transf.reshape(-1, 1)\n",
    "    X_val_transf = X_train_transf.reshape(-1, 1)\n",
    "    X_test_transf = X_test_transf.reshape(-1, 1)\n",
    "\n",
    "y_train = y_train.numpy()\n",
    "reg = LogisticRegression().fit(X_train_transf, y_train)\n",
    "\n",
    "\n",
    "train_loss = log_loss(y_test, reg.predict(X_test_transf).squeeze())\n",
    "val_loss = log_loss(y_val, reg.predict(X_val_transf).squeeze())\n",
    "test_loss = log_loss(y_test, reg.predict(X_test_transf).squeeze())\n",
    "\n",
    "print(\"Train Loss: \", train_loss)\n",
    "print(\"Val Loss: \", val_loss)\n",
    "print(\"Test Loss: \", test_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
